{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUbwnYZxT7HC",
        "outputId": "c846b350-cebf-409c-b79b-460f102b9772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.3MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 480kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.48MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.22MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 28, 28]             144\n",
            "       BatchNorm2d-2           [-1, 16, 28, 28]              32\n",
            "              ReLU-3           [-1, 16, 28, 28]               0\n",
            "           Dropout-4           [-1, 16, 28, 28]               0\n",
            "            Conv2d-5           [-1, 16, 28, 28]           2,304\n",
            "       BatchNorm2d-6           [-1, 16, 28, 28]              32\n",
            "              ReLU-7           [-1, 16, 28, 28]               0\n",
            "           Dropout-8           [-1, 16, 28, 28]               0\n",
            "         MaxPool2d-9           [-1, 16, 14, 14]               0\n",
            "           Conv2d-10           [-1, 16, 14, 14]             256\n",
            "      BatchNorm2d-11           [-1, 16, 14, 14]              32\n",
            "             ReLU-12           [-1, 16, 14, 14]               0\n",
            "           Conv2d-13           [-1, 16, 14, 14]           2,304\n",
            "      BatchNorm2d-14           [-1, 16, 14, 14]              32\n",
            "             ReLU-15           [-1, 16, 14, 14]               0\n",
            "          Dropout-16           [-1, 16, 14, 14]               0\n",
            "           Conv2d-17           [-1, 32, 14, 14]           4,608\n",
            "      BatchNorm2d-18           [-1, 32, 14, 14]              64\n",
            "             ReLU-19           [-1, 32, 14, 14]               0\n",
            "          Dropout-20           [-1, 32, 14, 14]               0\n",
            "AdaptiveAvgPool2d-21             [-1, 32, 1, 1]               0\n",
            "           Conv2d-22             [-1, 10, 1, 1]             320\n",
            "================================================================\n",
            "Total params: 10,128\n",
            "Trainable params: 10,128\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.15\n",
            "Params size (MB): 0.04\n",
            "Estimated Total Size (MB): 1.19\n",
            "----------------------------------------------------------------\n",
            "Epoch 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=1.5498 LR=0.004162 Batch_id=468 Accuracy=28.17: 100%|██████████| 469/469 [00:19<00:00, 24.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 1.5724, Accuracy: 4866/10000 (48.66%)\n",
            "\n",
            "Epoch 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.6218 LR=0.008123 Batch_id=468 Accuracy=75.01: 100%|██████████| 469/469 [00:18<00:00, 25.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.6238, Accuracy: 7952/10000 (79.52%)\n",
            "\n",
            "Epoch 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.2247 LR=0.012085 Batch_id=468 Accuracy=92.18: 100%|██████████| 469/469 [00:18<00:00, 25.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.5825, Accuracy: 7983/10000 (79.83%)\n",
            "\n",
            "Epoch 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.2203 LR=0.016047 Batch_id=468 Accuracy=94.37: 100%|██████████| 469/469 [00:19<00:00, 24.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.3988, Accuracy: 8712/10000 (87.12%)\n",
            "\n",
            "Epoch 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.1390 LR=0.019997 Batch_id=468 Accuracy=95.17: 100%|██████████| 469/469 [00:17<00:00, 26.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1748, Accuracy: 9441/10000 (94.41%)\n",
            "\n",
            "Epoch 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.1449 LR=0.018664 Batch_id=468 Accuracy=95.94: 100%|██████████| 469/469 [00:19<00:00, 24.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.1471, Accuracy: 9584/10000 (95.84%)\n",
            "\n",
            "Epoch 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.1047 LR=0.017331 Batch_id=468 Accuracy=96.34: 100%|██████████| 469/469 [00:17<00:00, 26.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0880, Accuracy: 9750/10000 (97.50%)\n",
            "\n",
            "Epoch 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.1440 LR=0.015998 Batch_id=468 Accuracy=96.71: 100%|██████████| 469/469 [00:18<00:00, 25.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0920, Accuracy: 9726/10000 (97.26%)\n",
            "\n",
            "Epoch 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.1496 LR=0.014664 Batch_id=468 Accuracy=96.95: 100%|██████████| 469/469 [00:19<00:00, 23.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0809, Accuracy: 9758/10000 (97.58%)\n",
            "\n",
            "Epoch 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0723 LR=0.013331 Batch_id=468 Accuracy=97.06: 100%|██████████| 469/469 [00:17<00:00, 26.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0654, Accuracy: 9810/10000 (98.10%)\n",
            "\n",
            "Epoch 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0801 LR=0.011998 Batch_id=468 Accuracy=97.28: 100%|██████████| 469/469 [00:19<00:00, 24.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0897, Accuracy: 9717/10000 (97.17%)\n",
            "\n",
            "Epoch 12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0920 LR=0.010665 Batch_id=468 Accuracy=97.36: 100%|██████████| 469/469 [00:18<00:00, 25.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0793, Accuracy: 9745/10000 (97.45%)\n",
            "\n",
            "Epoch 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0324 LR=0.009332 Batch_id=468 Accuracy=97.42: 100%|██████████| 469/469 [00:18<00:00, 24.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0619, Accuracy: 9812/10000 (98.12%)\n",
            "\n",
            "Epoch 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0475 LR=0.007998 Batch_id=468 Accuracy=97.55: 100%|██████████| 469/469 [00:19<00:00, 24.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0604, Accuracy: 9796/10000 (97.96%)\n",
            "\n",
            "Epoch 15\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0384 LR=0.006665 Batch_id=468 Accuracy=97.65: 100%|██████████| 469/469 [00:18<00:00, 25.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0612, Accuracy: 9802/10000 (98.02%)\n",
            "\n",
            "Epoch 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.1193 LR=0.005332 Batch_id=468 Accuracy=97.66: 100%|██████████| 469/469 [00:18<00:00, 24.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0551, Accuracy: 9822/10000 (98.22%)\n",
            "\n",
            "Epoch 17\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.1317 LR=0.003999 Batch_id=468 Accuracy=97.81: 100%|██████████| 469/469 [00:18<00:00, 25.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0537, Accuracy: 9825/10000 (98.25%)\n",
            "\n",
            "Epoch 18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0897 LR=0.002666 Batch_id=468 Accuracy=97.93: 100%|██████████| 469/469 [00:19<00:00, 24.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0445, Accuracy: 9860/10000 (98.60%)\n",
            "\n",
            "Epoch 19\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0558 LR=0.001332 Batch_id=468 Accuracy=97.99: 100%|██████████| 469/469 [00:18<00:00, 25.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0535, Accuracy: 9834/10000 (98.34%)\n",
            "\n",
            "Epoch 20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loss=0.0347 LR=-0.000001 Batch_id=468 Accuracy=98.08: 100%|██████████| 469/469 [00:18<00:00, 25.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.0512, Accuracy: 9840/10000 (98.40%)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomAffine(degrees=7, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST mean and std\n",
        "    ])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "kwargs = {'num_workers': 4, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True, transform=train_transforms),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, download=True, transform=test_transforms),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.convblock1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05)\n",
        "        ) # output_size = 28\n",
        "\n",
        "        self.convblock2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        ) # output_size = 28\n",
        "\n",
        "        \n",
        "        self.pool1 = nn.MaxPool2d(2, 2) # output_size = 14\n",
        "        self.convblock3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU()\n",
        "        ) # output_size = 14\n",
        "\n",
        "        \n",
        "        self.convblock4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        ) # output_size = 14\n",
        "        self.convblock5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        ) # output_size = 14\n",
        "\n",
        "        \n",
        "        self.gap = nn.AdaptiveAvgPool2d(1) \n",
        "        self.convblock6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=10, kernel_size=1, padding=0, bias=False)\n",
        "        ) # output_size = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convblock1(x)\n",
        "        x = self.convblock2(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.convblock3(x)\n",
        "        x = self.convblock4(x)\n",
        "        x = self.convblock5(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.convblock6(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))\n",
        "\n",
        "def train(model, device, train_loader, optimizer, scheduler):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step() \n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "        pbar.set_description(desc= f'Loss={loss.item():.4f} LR={scheduler.get_last_lr()[0]:.6f} Batch_id={batch_idx} Accuracy={100*correct/processed:0.2f}')\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.2f}%)\\n')\n",
        "    return (100. * correct / len(test_loader.dataset))\n",
        "\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "EPOCHS = 20\n",
        "\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=0.02, \n",
        "    steps_per_epoch=len(train_loader),\n",
        "    epochs=EPOCHS,\n",
        "    pct_start=5/EPOCHS, \n",
        "    div_factor=100, \n",
        "    three_phase=False,\n",
        "    final_div_factor=100, \n",
        "    anneal_strategy='linear'\n",
        ")\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    print(f\"Epoch {epoch}\")\n",
        "    train(model, device, train_loader, optimizer, scheduler)\n",
        "    test(model, device, test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
